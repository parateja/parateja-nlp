{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St4BIutUdFZ7",
        "outputId": "bab8a467-8878-4f27-defd-4856f750de52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " my name is jithendra teja i am studying B.tech third year\n",
            " my name is jithendra teja i am studying B.tech third year\n",
            " \n",
            "my\n",
            "name\n",
            "is\n",
            "jithendra\n",
            "teja\n",
            "i\n",
            "am\n",
            "studying\n",
            "B.tech\n",
            "third\n",
            "year\n"
          ]
        }
      ],
      "source": [
        "#exp-1(word tokenization)\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\" my name is jithendra teja i am studying B.tech third year\")\n",
        "\n",
        "print(doc)\n",
        "for s in doc.sents:\n",
        "  print(s)\n",
        "\n",
        "for s in doc.sents:\n",
        "  for w in s:\n",
        "    print(w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-2(word generation)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence=\"the boy is drinking milk\"\n",
        "word=word_tokenize(sentence)\n",
        "print(word)\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tags=nltk.pos_tag(word)\n",
        "print(tags)\n",
        "\n",
        "# parse tree construction\n",
        "\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "chunker = RegexpParser(\"\"\"\n",
        "                    NP: {<DT>?<JJ>*<NN>}\n",
        "                    P: {<IN>}\n",
        "                    V: {<V.*>}\n",
        "                    PP: {<p> <NP>}\n",
        "                    VP: {<V> <NP|PP>*}\n",
        "                    \"\"\")\n",
        "\n",
        "output = chunker.parse(tags)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVQ9gxIOdstN",
        "outputId": "318bd5df-37db-4f61-e83a-58a53db829d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'boy', 'is', 'drinking', 'milk']\n",
            "[('the', 'DT'), ('boy', 'NN'), ('is', 'VBZ'), ('drinking', 'VBG'), ('milk', 'NN')]\n",
            "(S\n",
            "  (NP the/DT boy/NN)\n",
            "  (VP (V is/VBZ))\n",
            "  (VP (V drinking/VBG) (NP milk/NN)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-3(morphological analysis)\n",
        "\n",
        "import spacy\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "doc=nlp(\"pink rose\")\n",
        "for token in doc:\n",
        "  print(token,\"|\",token.morph)\n",
        "  print(token,\"|\",token.pos)\n",
        "  print(token,\"|\",token.pos_)\n",
        "  print(token,\"|\",token.tag_)\n",
        "  print(token,\"|\",spacy.explain(token.tag_))\n",
        "  print(token,\"|\",spacy.explain(token.tag))\n",
        "#stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer(language=\"english\")\n",
        "token=input(\"enter a token: \")\n",
        "stemmer.stem(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "6JUHjO8veO7n",
        "outputId": "790a7406-3e20-4dfc-a149-bfb8d56d3d7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '164681854541413346' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n",
            "/usr/local/lib/python3.10/dist-packages/spacy/glossary.py:20: UserWarning: [W118] Term '17109001835818727656' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
            "  warnings.warn(Warnings.W118.format(term=term))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pink | \n",
            "pink | 86\n",
            "pink | ADV\n",
            "pink | RB\n",
            "pink | adverb\n",
            "pink | None\n",
            "rose | Tense=Past|VerbForm=Fin\n",
            "rose | 100\n",
            "rose | VERB\n",
            "rose | VBD\n",
            "rose | verb, past tense\n",
            "rose | None\n",
            "enter a token: roses\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rose'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-4(N-gram)\n",
        "\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "sampletext='this is a very good book to study'\n",
        "ngrams=ngrams(sequence=nltk.word_tokenize(sampletext), n=3)\n",
        "for grams in ngrams:\n",
        "    print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aLVztTJekfe",
        "outputId": "8998fe75-5643-42f8-f933-6d525db257c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this', 'is', 'a')\n",
            "('is', 'a', 'very')\n",
            "('a', 'very', 'good')\n",
            "('very', 'good', 'book')\n",
            "('good', 'book', 'to')\n",
            "('book', 'to', 'study')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp-5(N-gram smoothing)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "# main module begin here\n",
        "\n",
        "sentences = \"This sntence contins errors. This sentence has to be corrcted.\"\n",
        "list_string = sentences.split(' ')\n",
        "for word in list_string:\n",
        "    print(correction(word))\n",
        "\n",
        "\n",
        "print(' '.join('<<'+i+'>>'+' %s'%correction(i) if correction(i) != i else i for i in sentences.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pJkMlxBe8es",
        "outputId": "7c176944-cc2f-48b7-9185-874a801c3da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this\n",
            "sntence\n",
            "contins\n",
            "errors\n",
            "this\n",
            "sentence\n",
            "has\n",
            "to\n",
            "be\n",
            "corrcted\n",
            "<<This>> this sntence contins <<errors.>> errors <<This>> this sentence has to be <<corrcted.>> corrcted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exp-6(hidden morkov model)\n",
        "\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Load the Brown corpus\n",
        "corpus = brown.tagged_sents()\n",
        "\n",
        "# Split corpus into train and test sets\n",
        "train_corpus = corpus[:int(0.8*len(corpus))]\n",
        "test_corpus = corpus[int(0.8*len(corpus)):]\n",
        "\n",
        "# Define the tagset\n",
        "tagset = set([tag for sentence in corpus for _, tag in sentence])\n",
        "\n",
        "# Define the HMM model\n",
        "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(tagset)\n",
        "\n",
        "# Train the model on the training set\n",
        "model = trainer.train_supervised(train_corpus)\n",
        "\n",
        "# Test the model on the test set\n",
        "accuracy = model.accuracy(test_corpus)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Use the model to tag a new sentence\n",
        "new_sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "tagged_sentence = model.tag(new_sentence)\n",
        "print(tagged_sentence)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owWPoYeTf1Od",
        "outputId": "6f26e8ed-1fb4-4608-968d-d10d45b0b6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5753693278838421\n",
            "[('The', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-7(viterbi algorithm)\n",
        "\n",
        "H=enumerate\n",
        "D=1.\n",
        "G=range\n",
        "B=.0\n",
        "import numpy as A\n",
        "\n",
        "def I(obs,states,start_p,trans_p,emit_p):\n",
        "\tM=emit_p;L=trans_p;K=states;H=obs;C=len(H);I=len(K);J=A.zeros((C,I));D=A.zeros((C,I));J[0,:]=0;D[0,:]=start_p*M[:,H[0]]\n",
        "\tfor B in G(1,C):\n",
        "\t\tfor E in G(I):D[B,E]=A.max(D[B-1,:]*L[:,E]*M[E,H[B]]);J[B,E]=A.argmax(D[B-1,:]*L[:,E])\n",
        "\tF=A.zeros(C);F[-1]=A.argmax(D[-1,:])\n",
        "\tfor B in G(C-2,-1,-1):F[B]=J[B+1,int(F[B+1])]\n",
        "\tN=[K[int(A)]for A in F];return N\n",
        "E='I like pizza'\n",
        "C=['PRP','VBP','NN']\n",
        "F={B:A for(A,B)in H(E.split())}\n",
        "F.update({B:A for(A,B)in H(C)})\n",
        "J=A.array([[.4,.6,B],[B,.4,.6],[B,B,D]])\n",
        "K=A.array([[.1,.4,.5],[B,.9,.1],[B,B,D]])\n",
        "L=[F[A]for A in E.split()]\n",
        "C=I(L,C,A.array([D,B,B]),J,K)\n",
        "print(C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnmjTQcjPo9g",
        "outputId": "da97566f-bf26-4ee8-9e5e-b570d0d96408"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRP', 'VBP', 'NN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-8(penn tree bank)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the Penn Treebank corpus and the Punkt tokenizer (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Given sentence\n",
        "sentence = \"The tagger produced good results.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the POS tagged tokens\n",
        "print(\"POS tagged tokens:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6erdwSAnvZJ",
        "outputId": "67c9f056-c38e-4e89-986b-b700ad175d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagged tokens:\n",
            "[('The', 'DT'), ('tagger', 'NN'), ('produced', 'VBD'), ('good', 'JJ'), ('results', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-9(chunked text)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sample_text= \"I am a coding ninja, and I am the best in coding.\"\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16 &') # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n",
        "os.environ['DISPLAY']=':1.0' # tell X clients to use our virtual DISPLAY :1.0.\n",
        "%matplotlib inline\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display\n",
        "tokenized=nltk.sent_tokenize(sample_text)\n",
        "for i in tokenized:\n",
        " words=nltk.word_tokenize(i)\n",
        " tagged_words=nltk.pos_tag(words)\n",
        " print(tagged_words)\n",
        " chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" # this is the grammar that we define,\n",
        " chunkParser=nltk.RegexpParser(chunkGram)\n",
        " chunked=chunkParser.parse(tagged_words)\n",
        " tree = Tree.fromstring(str(chunked))\n",
        " display(tree)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "d65xFkffqU5e",
        "outputId": "0c34c7ea-15fe-4bf8-b3cf-c164fc380866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('coding', 'NN'), ('ninja', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('the', 'DT'), ('best', 'JJS'), ('in', 'IN'), ('coding', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', ['I/PRP', 'am/VBP', 'a/DT', 'coding/NN', 'ninja/NN', ',/,', 'and/CC', 'I/PRP', 'am/VBP', 'the/DT', 'best/JJS', 'in/IN', 'coding/NN', './.'])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,888.0,72.0\" width=\"888px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"6.30631%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I/PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.15315%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"6.30631%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am/VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.90991%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.40541%\" x=\"13.5135%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">a/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.2162%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.90991%\" x=\"18.9189%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coding/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.8739%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.00901%\" x=\"28.8288%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ninja/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.3333%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.5045%\" x=\"37.8378%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,/,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.0901%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"42.3423%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and/CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.9459%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.30631%\" x=\"49.5495%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">I/PRP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.7027%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"55.8559%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">am/VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.4595%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.20721%\" x=\"63.0631%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the/DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.00901%\" x=\"70.2703%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">best/JJS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.7748%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.30631%\" x=\"79.2793%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in/IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.4324%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.90991%\" x=\"85.5856%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">coding/NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.5405%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.5045%\" x=\"95.4955%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">./.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.7477%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exp-10(chunk parser)\n",
        "\n",
        "D=print\n",
        "import nltk as A\n",
        "A.download('punkt')\n",
        "A.download('averaged_perceptron_tagger')\n",
        "A.download('conll2000')\n",
        "from nltk.corpus import conll2000 as E\n",
        "from nltk.chunk import conlltags2tree as F,tree2conlltags as G\n",
        "from nltk import ChunkParserI as H,TrigramTagger as I\n",
        "import random as J\n",
        "B=list(E.chunked_sents())\n",
        "J.shuffle(B)\n",
        "K=B[:int(len(B)*.9)]\n",
        "L=B[int(len(B)*.9+1):]\n",
        "class M(H):\n",
        "  D(C.evaluate(L))\n",
        "  N='He’s really into the spooky decorative light fixture.'\n",
        "  O=A.word_tokenize(N)\n",
        "  Q=C.parse(P)\n",
        "  D(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8aTW9iqQKnr",
        "outputId": "f8987509-cba8-42cb-8952-659f6bfb0579"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "<ipython-input-35-40ad9959dd22>:15: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  D(C.evaluate(L))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  88.2%%\n",
            "    Precision:     80.9%%\n",
            "    Recall:        84.6%%\n",
            "    F-Measure:     82.7%%\n",
            "(S\n",
            "  (NP He/PRP)\n",
            "  (VP ’/VBZ)\n",
            "  s/JJ\n",
            "  really/RB\n",
            "  (PP into/IN)\n",
            "  (NP the/DT spooky/JJ decorative/JJ light/NN fixture/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}